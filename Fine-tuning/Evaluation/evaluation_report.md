# ABAP Documentation Evaluation Report

**Generated**: 2025-06-08 23:43:22

## Summary

This report compares the performance of fine-tuned Llama 2.5 8B, base Llama 2.5 8B, and Gemini 2.0 Flash on ABAP documentation generation under 8K context constraints.

## Automated Metrics Results

### Base Llama
- **BLEU-4**: 0.2221
- **ROUGE-L**: 0.2662
- **BERTScore F1**: 0.8200
- **Semantic Similarity**: 0.4909
- **ABAP Coverage**: 0.8443

### Fine Tuned Llama
- **BLEU-4**: 0.2401
- **ROUGE-L**: 0.2701
- **BERTScore F1**: 0.8200
- **Semantic Similarity**: 0.4909
- **ABAP Coverage**: 0.8110

### Gemini
- **BLEU-4**: 0.1539
- **ROUGE-L**: 0.2840
- **BERTScore F1**: 0.8623
- **Semantic Similarity**: 0.7071
- **ABAP Coverage**: 0.8310

## LLM Evaluation Results

### Base Llama
- **Overall Score**: 3.34/5
- **Accuracy**: 3.20/5
- **ABAP Specificity**: 3.80/5
- **Recommendation Rate**: 0.0%

### Fine Tuned Llama
- **Overall Score**: 3.11/5
- **Accuracy**: 2.40/5
- **ABAP Specificity**: 4.00/5
- **Recommendation Rate**: 0.0%

### Gemini
- **Overall Score**: 3.09/5
- **Accuracy**: 2.60/5
- **ABAP Specificity**: 3.40/5
- **Recommendation Rate**: 0.0%

## Key Findings

- **Best Automated Metrics**: Fine Tuned Llama
- **Best LLM Evaluation**: Base Llama

## Conclusion

The evaluation demonstrates the effectiveness of fine-tuning for ABAP-specific documentation generation. 
All models were fairly compared under identical 8K token context constraints.

---
*Generated by ABAP Documentation Evaluation Framework*
